{"file_contents":{"group4/REFER/agent/calculate_hash.py":{"content":"import hashlib\r\nimport json\r\nimport os\r\nimport socket\r\nimport time\r\n\r\nfrom pymongo import MongoClient\r\nfrom watchdog.observers import Observer\r\nfrom watchdog.events import FileSystemEventHandler\r\n\r\n# ============================\r\n# Config & Mongo setup\r\n# ============================\r\n\r\nCONFIG_PATH = r\"config.json\"  # <-- change if needed\r\n\r\n\r\ndef load_config(path: str = CONFIG_PATH) -> dict:\r\n    if not os.path.exists(path):\r\n        raise FileNotFoundError(f\"Config file not found: {path}\")\r\n    with open(path, \"r\") as f:\r\n        cfg = json.load(f)\r\n\r\n    if \"mongo_uri\" not in cfg:\r\n        raise ValueError(\"Config is missing 'mongo_uri'\")\r\n\r\n    return cfg\r\n\r\n\r\nconfig = load_config()\r\n\r\nMONGO_URI = config[\"mongo_uri\"]\r\nDB_NAME = config.get(\"db_name\", \"fim\")\r\nCOLLECTION_NAME = config.get(\"collection_name\", \"events\")\r\nAGENT_ID = config.get(\"agent_id\", socket.gethostname())\r\nWATCH_DIR = config.get(\"watch_dir\", r\"C:\\Users\\Public\")\r\n\r\nmongo_client = MongoClient(MONGO_URI)\r\nmongo_collection = mongo_client[DB_NAME][COLLECTION_NAME]\r\n\r\n# ============================\r\n# Local DB files (JSON)\r\n# ============================\r\n\r\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\r\nHASH_DB_FILE = os.path.join(BASE_DIR, \"hashes.json\")\r\nHISTORY_DB_FILE = os.path.join(BASE_DIR, \"hash_history.json\")\r\n\r\n\r\ndef load_json(path: str):\r\n    if not os.path.exists(path):\r\n        return {}\r\n    try:\r\n        with open(path, \"r\") as f:\r\n            return json.load(f)\r\n    except json.JSONDecodeError:\r\n        return {}\r\n\r\n\r\ndef save_json(obj: dict, path: str):\r\n    with open(path, \"w\") as f:\r\n        json.dump(obj, f, indent=4)\r\n\r\n\r\n# ============================\r\n# Helpers: ignore temp files\r\n# ============================\r\n\r\ndef is_temp_file(path: str) -> bool:\r\n    \"\"\"Return True if the file should be ignored (e.g., editor temp files).\"\"\"\r\n    filename = os.path.basename(path)\r\n    # Only rule requested: skip files ending with \"~\"\r\n    return filename.endswith(\"~\")\r\n\r\n\r\n# ============================\r\n# Metadata & hashing (Windows)\r\n# ============================\r\n\r\ndef get_file_metadata(path: str) -> dict:\r\n    s = os.stat(path, follow_symlinks=False)\r\n    return {\r\n        \"size\": s.st_size,\r\n        \"mtime\": int(s.st_mtime),   # last modification time\r\n        \"ctime\": int(s.st_ctime),   # creation time on Windows\r\n        \"readonly\": not os.access(path, os.W_OK),\r\n    }\r\n\r\n\r\ndef hash_content(path: str) -> str:\r\n    h = hashlib.sha256()\r\n    with open(path, \"rb\") as f:\r\n        for chunk in iter(lambda: f.read(8192), b\"\"):\r\n            h.update(chunk)\r\n    return h.hexdigest()\r\n\r\n\r\ndef hash_state(path: str) -> dict:\r\n    meta = get_file_metadata(path)\r\n    content_hash = hash_content(path)\r\n\r\n    state_obj = {\r\n        \"path\": os.path.abspath(path),\r\n        \"content_hash\": content_hash,\r\n        \"metadata\": meta,\r\n    }\r\n\r\n    # Stable encoding for state_hash\r\n    state_bytes = json.dumps(state_obj, sort_keys=True,\r\n                             separators=(\",\", \":\")).encode()\r\n    state_hash = hashlib.sha256(state_bytes).hexdigest()\r\n\r\n    return {\r\n        \"path\": state_obj[\"path\"],\r\n        \"content_hash\": content_hash,\r\n        \"metadata\": meta,\r\n        \"state_hash\": state_hash,\r\n    }\r\n\r\n\r\n# ============================\r\n# History helpers\r\n# ============================\r\n\r\ndef append_history_entry(state: dict, history_db: dict, timestamp: int):\r\n    \"\"\"\r\n    Append a state entry to history, but only if the state_hash differs from\r\n    the last recorded one (no duplicate NO_CHANGE entries).\r\n    \"\"\"\r\n    path = state[\"path\"]\r\n    new_hash = state[\"state_hash\"]\r\n\r\n    entry = {\r\n        \"timestamp\": timestamp,\r\n        \"state_hash\": new_hash,\r\n        \"content_hash\": state[\"content_hash\"],\r\n        \"metadata\": state[\"metadata\"],\r\n    }\r\n\r\n    if path not in history_db:\r\n        history_db[path] = [entry]\r\n        return\r\n\r\n    last_entry = history_db[path][-1]\r\n    last_hash = last_entry.get(\"state_hash\")\r\n\r\n    if last_hash == new_hash:\r\n        # No change in state since last history entry\r\n        return\r\n\r\n    history_db[path].append(entry)\r\n\r\n\r\ndef append_deletion_history(path: str, history_db: dict, timestamp: int):\r\n    \"\"\"\r\n    Append a DELETED event to history, but avoid repeated DELETED entries.\r\n    \"\"\"\r\n    if path not in history_db:\r\n        history_db[path] = []\r\n\r\n    if history_db[path] and history_db[path][-1].get(\"event\") == \"DELETED\":\r\n        return\r\n\r\n    deletion_entry = {\r\n        \"timestamp\": timestamp,\r\n        \"state_hash\": None,\r\n        \"content_hash\": None,\r\n        \"metadata\": None,\r\n        \"event\": \"DELETED\",\r\n    }\r\n    history_db[path].append(deletion_entry)\r\n\r\n\r\n# ============================\r\n# Mongo event sender\r\n# ============================\r\n\r\ndef send_event_to_mongo(event: dict):\r\n    \"\"\"\r\n    Insert a single FIM event document into MongoDB.\r\n    \"\"\"\r\n    try:\r\n        mongo_collection.insert_one(event)\r\n    except Exception as e:\r\n        # Don't kill the agent if MongoDB is down\r\n        print(f\"[!] Failed to send event to MongoDB: {e}\")\r\n\r\n\r\n# ============================\r\n# File system event handler\r\n# ============================\r\n\r\nclass FIMEventHandler(FileSystemEventHandler):\r\n    def __init__(self, root_dir: str):\r\n        super().__init__()\r\n        self.root_dir = os.path.abspath(root_dir)\r\n\r\n    def _handle_file_change(self, path: str, event_type: str):\r\n        # Ignore directories explicitly\r\n        if os.path.isdir(path):\r\n            return\r\n\r\n        abs_path = os.path.abspath(path)\r\n        now_ts = int(time.time())\r\n\r\n        # Load DBs\r\n        db = load_json(HASH_DB_FILE)\r\n        history_db = load_json(HISTORY_DB_FILE)\r\n\r\n        # Handle deletion\r\n        if event_type == \"DELETED\":\r\n            print(f\"[DELETED] {abs_path}\")\r\n\r\n            # 1. Add to history (single DELETED)\r\n            append_deletion_history(abs_path, history_db, now_ts)\r\n\r\n            # 2. Remove from current snapshot DB\r\n            if abs_path in db:\r\n                del db[abs_path]\r\n\r\n            # 3. Send deletion event to Mongo\r\n            event_doc = {\r\n                \"timestamp\": now_ts,\r\n                \"event_type\": \"DELETED\",\r\n                \"path\": abs_path,\r\n                \"state_hash\": None,\r\n                \"content_hash\": None,\r\n                \"metadata\": None,\r\n                \"agent_id\": AGENT_ID,\r\n            }\r\n            send_event_to_mongo(event_doc)\r\n\r\n            # 4. Persist local DBs\r\n            save_json(db, HASH_DB_FILE)\r\n            save_json(history_db, HISTORY_DB_FILE)\r\n            return\r\n\r\n        # For CREATED / MODIFIED / MOVED target:\r\n        if not os.path.exists(abs_path):\r\n            # race condition: event fired but file already gone\r\n            return\r\n\r\n        # Calculate current state\r\n        try:\r\n            current_state = hash_state(abs_path)\r\n        except (PermissionError, FileNotFoundError):\r\n            # Can't read file for some reason\r\n            return\r\n\r\n        old_state = db.get(abs_path)\r\n\r\n        if old_state is None:\r\n            # never seen before\r\n            status = \"CREATED\"\r\n        else:\r\n            # if state_hash is the same, ignore the event (NO_CHANGE)\r\n            if current_state[\"state_hash\"] == old_state.get(\"state_hash\"):\r\n                return\r\n            status = \"MODIFIED\"\r\n\r\n        # Update snapshot & history\r\n        db[abs_path] = current_state\r\n        append_history_entry(current_state, history_db, now_ts)\r\n\r\n        # Build and send Mongo event\r\n        event_doc = {\r\n            \"timestamp\": now_ts,\r\n            \"event_type\": status,  # CREATED or MODIFIED\r\n            \"path\": abs_path,\r\n            \"state_hash\": current_state[\"state_hash\"],\r\n            \"content_hash\": current_state[\"content_hash\"],\r\n            \"metadata\": current_state[\"metadata\"],\r\n            \"agent_id\": AGENT_ID,\r\n        }\r\n        # send_event_to_mongo(event_doc)\r\n\r\n        # Save local DBs\r\n        save_json(db, HASH_DB_FILE)\r\n        save_json(history_db, HISTORY_DB_FILE)\r\n\r\n        print(f\"[{status}] {abs_path}\")\r\n\r\n    # Watchdog callbacks\r\n\r\n    def on_created(self, event):\r\n        if not event.is_directory and not is_temp_file(event.src_path):\r\n            self._handle_file_change(event.src_path, \"CREATED\")\r\n\r\n    def on_modified(self, event):\r\n        if not event.is_directory and not is_temp_file(event.src_path):\r\n            self._handle_file_change(event.src_path, \"MODIFIED\")\r\n\r\n    def on_deleted(self, event):\r\n        if not event.is_directory and not is_temp_file(event.src_path):\r\n            self._handle_file_change(event.src_path, \"DELETED\")\r\n\r\n    def on_moved(self, event):\r\n        if not event.is_directory:\r\n            # source deletion\r\n            if not is_temp_file(event.src_path):\r\n                self._handle_file_change(event.src_path, \"DELETED\")\r\n            # destination creation\r\n            if not is_temp_file(event.dest_path):\r\n                self._handle_file_change(event.dest_path, \"CREATED\")\r\n\r\n\r\n# ============================\r\n# Agent runner\r\n# ============================\r\n\r\ndef run_agent(watch_dir: str, recursive: bool = True):\r\n    watch_dir = os.path.abspath(watch_dir)\r\n    print(f\"[*] Starting FIM agent on: {watch_dir}\")\r\n    print(f\"[*] Agent ID: {AGENT_ID}\")\r\n    print(\"[*] Press Ctrl+C to stop.\\n\")\r\n\r\n    event_handler = FIMEventHandler(watch_dir)\r\n    observer = Observer()\r\n    observer.schedule(event_handler, watch_dir, recursive=recursive)\r\n    observer.start()\r\n\r\n    try:\r\n        while True:\r\n            time.sleep(1)\r\n    except KeyboardInterrupt:\r\n        print(\"\\n[*] Stopping observer...\")\r\n        observer.stop()\r\n    observer.join()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run_agent(WATCH_DIR, recursive=True)\r\n","path":null,"size_bytes":9752,"size_tokens":null},"group4/fim1/fim/app.py":{"content":"\"\"\"Flask web dashboard\"\"\"\nimport os\nfrom flask import Flask, render_template, request, redirect, url_for, jsonify\nfrom .models import (\n    get_latest_events,\n    get_latest_events_filtered,\n    get_distinct_file_paths,\n    get_file_classification,\n    upsert_file_classification,\n    get_all_classifications,\n    get_distinct_endpoints\n)\nfrom .config import FLASK_HOST, FLASK_PORT\n\n# Get the root directory (parent of fim package)\nROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nTEMPLATE_DIR = os.path.join(ROOT_DIR, \"templates\")\n\napp = Flask(__name__, template_folder=TEMPLATE_DIR)\n\n\n@app.route(\"/\")\ndef index():\n    \"\"\"Main dashboard page with advanced filtering\"\"\"\n    # Get search query\n    search_query = request.args.get(\"search\", \"\").strip()\n    \n    # Get event types (can be multiple checkboxes)\n    event_types_param = request.args.get(\"types\", \"\")\n    if event_types_param:\n        event_types = [t.strip() for t in event_types_param.split(\",\") if t.strip()]\n        # Remove \"all\" if other types are selected\n        if \"all\" in event_types and len(event_types) > 1:\n            event_types.remove(\"all\")\n        elif \"all\" in event_types:\n            event_types = None\n    else:\n        event_types = None\n    \n    # Get search columns\n    search_columns_param = request.args.get(\"columns\", \"\")\n    if search_columns_param:\n        search_columns = [c.strip() for c in search_columns_param.split(\",\") if c.strip()]\n        if \"all\" in search_columns:\n            search_columns = None  # Search all columns\n    else:\n        search_columns = None\n    \n    # Fetch events with filtering\n    if search_query or (event_types and len(event_types) > 0):\n        events = get_latest_events_filtered(\n            limit=100,\n            event_types=event_types,\n            search_query=search_query if search_query else None,\n            search_columns=search_columns\n        )\n    else:\n        # Legacy support - check for single type parameter\n        event_type = request.args.get(\"type\", \"all\")\n        valid_types = [\"all\", \"created\", \"modified\", \"deleted\"]\n        if event_type not in valid_types:\n            event_type = \"all\"\n        events = get_latest_events(limit=100, event_type=event_type if event_type != \"all\" else None)\n    \n    # For backward compatibility and UI state\n    selected_types = event_types if event_types else []\n    if not selected_types:\n        # Check legacy type parameter\n        selected_type = request.args.get(\"type\", \"all\")\n        if selected_type == \"all\":\n            selected_types = [\"all\"]\n        else:\n            selected_types = [selected_type]\n            event_types = [selected_type]  # Set for filtering logic\n    \n    # Determine if advanced filter should be shown\n    has_active_filters = bool(search_query or (event_types and len(event_types) > 0 and event_types != [\"all\"]))\n    \n    return render_template(\n        \"index.html\",\n        events=events,\n        search_query=search_query,\n        selected_event_types=selected_types,\n        selected_search_columns=search_columns if search_columns else [\"all\"],\n        has_active_filters=has_active_filters\n    )\n\n\n@app.route(\"/classification/save-all\", methods=[\"POST\"])\ndef classification_save_all():\n    \"\"\"AJAX endpoint to save all classifications at once\"\"\"\n    import json\n    \n    files_json = request.form.get(\"files\")\n    if not files_json:\n        return jsonify({\"success\": False, \"message\": \"Missing files parameter\"}), 400\n    \n    try:\n        files = json.loads(files_json)\n    except json.JSONDecodeError:\n        return jsonify({\"success\": False, \"message\": \"Invalid JSON format\"}), 400\n    \n    if not isinstance(files, list):\n        return jsonify({\"success\": False, \"message\": \"Files must be a list\"}), 400\n    \n    from .models import DB_PATH\n    import sqlite3\n    \n    conn = sqlite3.connect(DB_PATH)\n    cursor = conn.cursor()\n    \n    saved_count = 0\n    for file_data in files:\n        file_path = file_data.get(\"file_path\")\n        classification = file_data.get(\"classification\", \"\").strip()\n        endpoint = file_data.get(\"endpoint\")\n        hostname = file_data.get(\"hostname\")\n        username = file_data.get(\"username\")\n        \n        if not file_path:\n            continue\n        \n        if not classification:\n            # Delete record if classification is empty\n            cursor.execute(\"DELETE FROM file_classification WHERE file_path = ?\", (file_path,))\n        else:\n            # Update or insert classification\n            from datetime import datetime\n            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n            \n            # Check if record exists\n            cursor.execute(\"SELECT id FROM file_classification WHERE file_path = ?\", (file_path,))\n            existing = cursor.fetchone()\n            \n            if existing:\n                # Update existing record\n                cursor.execute(\"\"\"\n                    UPDATE file_classification SET\n                        classification = ?,\n                        last_updated_timestamp = ?,\n                        endpoint = ?,\n                        hostname = ?,\n                        username = ?\n                    WHERE file_path = ?\n                \"\"\", (classification, timestamp, endpoint, hostname, username, file_path))\n            else:\n                # Insert new record\n                cursor.execute(\"\"\"\n                    INSERT INTO file_classification (\n                        file_path, classification, last_updated_timestamp,\n                        endpoint, hostname, username\n                    ) VALUES (?, ?, ?, ?, ?, ?)\n                \"\"\", (file_path, classification, timestamp, endpoint, hostname, username))\n        \n        saved_count += 1\n    \n    conn.commit()\n    conn.close()\n    \n    return jsonify({\n        \"success\": True,\n        \"message\": f\"Successfully saved {saved_count} classification(s)\"\n    })\n\n\n@app.route(\"/classification/update\", methods=[\"POST\"])\ndef classification_update():\n    \"\"\"AJAX endpoint to update classification without page reload\"\"\"\n    file_path = request.form.get(\"file_path\")\n    classification = request.form.get(\"classification\", \"\").strip()\n    endpoint = request.form.get(\"endpoint\")\n    hostname = request.form.get(\"hostname\")\n    username = request.form.get(\"username\")\n    \n    if not file_path:\n        return jsonify({\"success\": False, \"message\": \"Missing file_path parameter\"}), 400\n    \n    # If classification is empty, delete the record to clear it\n    if not classification:\n        from .models import DB_PATH\n        import sqlite3\n        conn = sqlite3.connect(DB_PATH)\n        cursor = conn.cursor()\n        cursor.execute(\"DELETE FROM file_classification WHERE file_path = ?\", (file_path,))\n        conn.commit()\n        conn.close()\n        return jsonify({\"success\": True, \"message\": \"Classification cleared successfully\"})\n    \n    # Update or insert classification\n    upsert_file_classification(\n        file_path=file_path,\n        classification=classification,\n        endpoint=endpoint,\n        hostname=hostname,\n        username=username\n    )\n    \n    return jsonify({\"success\": True, \"message\": \"Classification updated successfully\"})\n\n\n@app.route(\"/classification\", methods=[\"GET\"])\ndef classification():\n    \"\"\"Classification page for assigning security levels to files\"\"\"\n    \n    # GET request - display classification page\n    # Get endpoint filters\n    endpoints_param = request.args.get(\"endpoints\", \"\")\n    if endpoints_param:\n        selected_endpoints = [e.strip() for e in endpoints_param.split(\",\") if e.strip()]\n    else:\n        selected_endpoints = None\n    \n    # Get search query for file path\n    search_query = request.args.get(\"search\", \"\").strip()\n    \n    # Get distinct file paths\n    files = get_distinct_file_paths(endpoints=selected_endpoints)\n    \n    # Filter by search query if provided\n    if search_query:\n        files = [f for f in files if search_query.lower() in f[\"file_path\"].lower()]\n    \n    # Get existing classifications\n    classifications_dict = {}\n    if selected_endpoints:\n        classifications = get_all_classifications(endpoints=selected_endpoints)\n    else:\n        classifications = get_all_classifications()\n    \n    for cls in classifications:\n        classifications_dict[cls[\"file_path\"]] = cls\n    \n    # Merge file info with classifications\n    for file_info in files:\n        file_path = file_info[\"file_path\"]\n        if file_path in classifications_dict:\n            file_info[\"classification\"] = classifications_dict[file_path][\"classification\"]\n            file_info[\"classification_id\"] = classifications_dict[file_path][\"id\"]\n        else:\n            file_info[\"classification\"] = None\n            file_info[\"classification_id\"] = None\n    \n    # Get available endpoints for filter\n    available_endpoints = get_distinct_endpoints()\n    \n    return render_template(\n        \"classification.html\",\n        files=files,\n        available_endpoints=available_endpoints,\n        selected_endpoints=selected_endpoints if selected_endpoints else [],\n        search_query=search_query\n    )\n\n\ndef run_app(host: str = None, port: int = None, debug: bool = False) -> None:\n    \"\"\"Run the Flask application\n    \n    Args:\n        host: Host to bind to (defaults to config value)\n        port: Port to bind to (defaults to config value)\n        debug: Enable debug mode\n    \"\"\"\n    host = host or FLASK_HOST\n    port = port or FLASK_PORT\n    \n    print(f\"[FLASK] Starting dashboard on http://{host}:{port}\")\n    app.run(host=host, port=port, debug=debug, use_reloader=False)\n\n","path":null,"size_bytes":9616,"size_tokens":null},"README.md":{"content":"# group04","path":null,"size_bytes":9,"size_tokens":null},"group4/fim1/fim/models.py":{"content":"\"\"\"Database models and data access layer\"\"\"\nimport sqlite3\nimport os\nfrom typing import Optional, List, Dict, Any\nfrom .config import DB_PATH, DATA_DIR\n\n\ndef init_db() -> None:\n    \"\"\"Create the database and events table if they don't exist\"\"\"\n    # Ensure data directory exists\n    os.makedirs(DATA_DIR, exist_ok=True)\n    \n    conn = sqlite3.connect(DB_PATH)\n    cursor = conn.cursor()\n    \n    cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS events (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            event_type TEXT NOT NULL,\n            file_path TEXT NOT NULL,\n            timestamp TEXT NOT NULL,\n            endpoint TEXT NOT NULL,\n            hostname TEXT NOT NULL,\n            username TEXT NOT NULL,\n            hash_before TEXT,\n            hash_after TEXT\n        )\n    \"\"\")\n    \n    # Create index on event_type for faster filtering\n    cursor.execute(\"\"\"\n        CREATE INDEX IF NOT EXISTS idx_event_type \n        ON events(event_type)\n    \"\"\")\n    \n    # Create index on timestamp for faster ordering\n    cursor.execute(\"\"\"\n        CREATE INDEX IF NOT EXISTS idx_timestamp \n        ON events(timestamp)\n    \"\"\")\n    \n    # Create file_classification table\n    cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS file_classification (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            file_path TEXT NOT NULL UNIQUE,\n            classification TEXT NOT NULL,\n            last_updated_timestamp TEXT NOT NULL,\n            endpoint TEXT,\n            hostname TEXT,\n            username TEXT\n        )\n    \"\"\")\n    \n    # Create index on file_path for faster lookups\n    cursor.execute(\"\"\"\n        CREATE INDEX IF NOT EXISTS idx_file_path \n        ON file_classification(file_path)\n    \"\"\")\n    \n    conn.commit()\n    conn.close()\n\n\ndef insert_event(data: Dict[str, Any]) -> int:\n    \"\"\"Insert a single event into the database\n    \n    Args:\n        data: Dictionary with keys: event_type, file_path, timestamp,\n              endpoint, hostname, username, hash_before, hash_after\n    \n    Returns:\n        The ID of the inserted event\n    \"\"\"\n    conn = sqlite3.connect(DB_PATH)\n    cursor = conn.cursor()\n    \n    cursor.execute(\"\"\"\n        INSERT INTO events (\n            event_type, file_path, timestamp, endpoint,\n            hostname, username, hash_before, hash_after\n        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n    \"\"\", (\n        data.get(\"event_type\"),\n        data.get(\"file_path\"),\n        data.get(\"timestamp\"),\n        data.get(\"endpoint\"),\n        data.get(\"hostname\"),\n        data.get(\"username\"),\n        data.get(\"hash_before\"),\n        data.get(\"hash_after\"),\n    ))\n    \n    event_id = cursor.lastrowid\n    conn.commit()\n    conn.close()\n    \n    return event_id\n\n\ndef get_latest_hash(file_path: str) -> Optional[str]:\n    \"\"\"Get the most recent hash_after for a given file path\n    \n    Args:\n        file_path: Path to the file\n    \n    Returns:\n        The most recent hash_after value, or None if not found\n    \"\"\"\n    conn = sqlite3.connect(DB_PATH)\n    cursor = conn.cursor()\n    \n    cursor.execute(\"\"\"\n        SELECT hash_after FROM events\n        WHERE file_path = ? AND hash_after IS NOT NULL\n        ORDER BY timestamp DESC\n        LIMIT 1\n    \"\"\", (file_path,))\n    \n    result = cursor.fetchone()\n    conn.close()\n    \n    return result[0] if result else None\n\n\ndef get_latest_events(limit: int = 100, event_type: Optional[str] = None) -> List[Dict[str, Any]]:\n    \"\"\"Get the latest events from the database\n    \n    Args:\n        limit: Maximum number of events to return\n        event_type: Optional filter by event type (created/modified/deleted)\n    \n    Returns:\n        List of event dictionaries ordered by timestamp (newest first)\n    \"\"\"\n    conn = sqlite3.connect(DB_PATH)\n    conn.row_factory = sqlite3.Row\n    cursor = conn.cursor()\n    \n    if event_type and event_type != \"all\":\n        cursor.execute(\"\"\"\n            SELECT * FROM events\n            WHERE event_type = ?\n            ORDER BY timestamp DESC\n            LIMIT ?\n        \"\"\", (event_type, limit))\n    else:\n        cursor.execute(\"\"\"\n            SELECT * FROM events\n            ORDER BY timestamp DESC\n            LIMIT ?\n        \"\"\", (limit,))\n    \n    rows = cursor.fetchall()\n    conn.close()\n    \n    # Convert Row objects to dictionaries\n    events = []\n    for row in rows:\n        events.append({\n            \"id\": row[\"id\"],\n            \"event_type\": row[\"event_type\"],\n            \"file_path\": row[\"file_path\"],\n            \"timestamp\": row[\"timestamp\"],\n            \"endpoint\": row[\"endpoint\"],\n            \"hostname\": row[\"hostname\"],\n            \"username\": row[\"username\"],\n            \"hash_before\": row[\"hash_before\"],\n            \"hash_after\": row[\"hash_after\"],\n        })\n    \n    return events\n\n\ndef get_latest_events_filtered(\n    limit: int = 100,\n    event_types: Optional[List[str]] = None,\n    search_query: Optional[str] = None,\n    search_columns: Optional[List[str]] = None\n) -> List[Dict[str, Any]]:\n    \"\"\"Get the latest events with advanced filtering\n    \n    Args:\n        limit: Maximum number of events to return\n        event_types: Optional list of event types to filter (created/modified/deleted)\n        search_query: Optional search query string\n        search_columns: Optional list of columns to search in (if None, searches all)\n    \n    Returns:\n        List of event dictionaries ordered by timestamp (newest first)\n    \"\"\"\n    conn = sqlite3.connect(DB_PATH)\n    conn.row_factory = sqlite3.Row\n    cursor = conn.cursor()\n    \n    query = \"SELECT * FROM events WHERE 1=1\"\n    params = []\n    \n    # Filter by event types\n    if event_types and len(event_types) > 0:\n        placeholders = \",\".join(\"?\" * len(event_types))\n        query += f\" AND event_type IN ({placeholders})\"\n        params.extend(event_types)\n    \n    # Search query\n    if search_query and search_query.strip():\n        search_term = f\"%{search_query.strip()}%\"\n        if search_columns and len(search_columns) > 0:\n            # Search in specific columns\n            column_conditions = []\n            for col in search_columns:\n                if col in [\"timestamp\", \"event_type\", \"file_path\", \"endpoint\", \"hostname\", \"username\"]:\n                    column_conditions.append(f\"{col} LIKE ?\")\n                    params.append(search_term)\n            if column_conditions:\n                query += \" AND (\" + \" OR \".join(column_conditions) + \")\"\n        else:\n            # Search in all columns\n            query += \"\"\" AND (\n                timestamp LIKE ? OR\n                event_type LIKE ? OR\n                file_path LIKE ? OR\n                endpoint LIKE ? OR\n                hostname LIKE ? OR\n                username LIKE ?\n            )\"\"\"\n            params.extend([search_term] * 6)\n    \n    query += \" ORDER BY timestamp DESC LIMIT ?\"\n    params.append(limit)\n    \n    cursor.execute(query, params)\n    rows = cursor.fetchall()\n    conn.close()\n    \n    # Convert Row objects to dictionaries\n    events = []\n    for row in rows:\n        events.append({\n            \"id\": row[\"id\"],\n            \"event_type\": row[\"event_type\"],\n            \"file_path\": row[\"file_path\"],\n            \"timestamp\": row[\"timestamp\"],\n            \"endpoint\": row[\"endpoint\"],\n            \"hostname\": row[\"hostname\"],\n            \"username\": row[\"username\"],\n            \"hash_before\": row[\"hash_before\"],\n            \"hash_after\": row[\"hash_after\"],\n        })\n    \n    return events\n\n\ndef get_distinct_file_paths(endpoints: Optional[List[str]] = None) -> List[Dict[str, Any]]:\n    \"\"\"Get distinct file paths with their latest event information\n    \n    Args:\n        endpoints: Optional list of endpoints to filter by\n    \n    Returns:\n        List of dictionaries with file_path and latest event info\n    \"\"\"\n    conn = sqlite3.connect(DB_PATH)\n    conn.row_factory = sqlite3.Row\n    cursor = conn.cursor()\n    \n    query = \"\"\"\n        SELECT DISTINCT\n            e1.file_path,\n            e1.timestamp as last_timestamp,\n            e1.endpoint,\n            e1.hostname,\n            e1.username\n        FROM events e1\n        INNER JOIN (\n            SELECT file_path, MAX(timestamp) as max_timestamp\n            FROM events\n            GROUP BY file_path\n        ) e2 ON e1.file_path = e2.file_path AND e1.timestamp = e2.max_timestamp\n    \"\"\"\n    \n    params = []\n    if endpoints and len(endpoints) > 0:\n        placeholders = \",\".join(\"?\" * len(endpoints))\n        query += f\" WHERE e1.endpoint IN ({placeholders})\"\n        params.extend(endpoints)\n    \n    query += \" ORDER BY e1.timestamp DESC\"\n    \n    cursor.execute(query, params)\n    rows = cursor.fetchall()\n    conn.close()\n    \n    files = []\n    for row in rows:\n        files.append({\n            \"file_path\": row[\"file_path\"],\n            \"last_timestamp\": row[\"last_timestamp\"],\n            \"endpoint\": row[\"endpoint\"],\n            \"hostname\": row[\"hostname\"],\n            \"username\": row[\"username\"],\n        })\n    \n    return files\n\n\ndef get_file_classification(file_path: str) -> Optional[Dict[str, Any]]:\n    \"\"\"Get classification for a specific file path\n    \n    Args:\n        file_path: Path to the file\n    \n    Returns:\n        Classification dictionary or None if not found\n    \"\"\"\n    conn = sqlite3.connect(DB_PATH)\n    conn.row_factory = sqlite3.Row\n    cursor = conn.cursor()\n    \n    cursor.execute(\"\"\"\n        SELECT * FROM file_classification\n        WHERE file_path = ?\n    \"\"\", (file_path,))\n    \n    row = cursor.fetchone()\n    conn.close()\n    \n    if row:\n        return {\n            \"id\": row[\"id\"],\n            \"file_path\": row[\"file_path\"],\n            \"classification\": row[\"classification\"],\n            \"last_updated_timestamp\": row[\"last_updated_timestamp\"],\n            \"endpoint\": row[\"endpoint\"],\n            \"hostname\": row[\"hostname\"],\n            \"username\": row[\"username\"],\n        }\n    return None\n\n\ndef upsert_file_classification(\n    file_path: str,\n    classification: str,\n    endpoint: Optional[str] = None,\n    hostname: Optional[str] = None,\n    username: Optional[str] = None\n) -> int:\n    \"\"\"Insert or update file classification\n    \n    Args:\n        file_path: Path to the file\n        classification: Classification level (Top Secret, Secret, Confidential, Unclassified)\n        endpoint: Optional endpoint\n        hostname: Optional hostname\n        username: Optional username\n    \n    Returns:\n        The ID of the inserted/updated record\n    \"\"\"\n    from datetime import datetime\n    \n    conn = sqlite3.connect(DB_PATH)\n    cursor = conn.cursor()\n    \n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    \n    # Check if record exists\n    cursor.execute(\"SELECT id FROM file_classification WHERE file_path = ?\", (file_path,))\n    existing = cursor.fetchone()\n    \n    if existing:\n        # Update existing record\n        cursor.execute(\"\"\"\n            UPDATE file_classification SET\n                classification = ?,\n                last_updated_timestamp = ?,\n                endpoint = ?,\n                hostname = ?,\n                username = ?\n            WHERE file_path = ?\n        \"\"\", (classification, timestamp, endpoint, hostname, username, file_path))\n        record_id = existing[0]\n    else:\n        # Insert new record\n        cursor.execute(\"\"\"\n            INSERT INTO file_classification (\n                file_path, classification, last_updated_timestamp,\n                endpoint, hostname, username\n            ) VALUES (?, ?, ?, ?, ?, ?)\n        \"\"\", (file_path, classification, timestamp, endpoint, hostname, username))\n        record_id = cursor.lastrowid\n    \n    record_id = cursor.lastrowid\n    conn.commit()\n    conn.close()\n    \n    return record_id\n\n\ndef get_all_classifications(endpoints: Optional[List[str]] = None) -> List[Dict[str, Any]]:\n    \"\"\"Get all file classifications, optionally filtered by endpoints\n    \n    Args:\n        endpoints: Optional list of endpoints to filter by\n    \n    Returns:\n        List of classification dictionaries\n    \"\"\"\n    conn = sqlite3.connect(DB_PATH)\n    conn.row_factory = sqlite3.Row\n    cursor = conn.cursor()\n    \n    query = \"SELECT * FROM file_classification WHERE 1=1\"\n    params = []\n    \n    if endpoints and len(endpoints) > 0:\n        placeholders = \",\".join(\"?\" * len(endpoints))\n        query += f\" AND endpoint IN ({placeholders})\"\n        params.extend(endpoints)\n    \n    query += \" ORDER BY last_updated_timestamp DESC\"\n    \n    cursor.execute(query, params)\n    rows = cursor.fetchall()\n    conn.close()\n    \n    classifications = []\n    for row in rows:\n        classifications.append({\n            \"id\": row[\"id\"],\n            \"file_path\": row[\"file_path\"],\n            \"classification\": row[\"classification\"],\n            \"last_updated_timestamp\": row[\"last_updated_timestamp\"],\n            \"endpoint\": row[\"endpoint\"],\n            \"hostname\": row[\"hostname\"],\n            \"username\": row[\"username\"],\n        })\n    \n    return classifications\n\n\ndef get_distinct_endpoints() -> List[str]:\n    \"\"\"Get list of distinct endpoints from events table\n    \n    Returns:\n        List of endpoint strings\n    \"\"\"\n    conn = sqlite3.connect(DB_PATH)\n    cursor = conn.cursor()\n    \n    cursor.execute(\"SELECT DISTINCT endpoint FROM events ORDER BY endpoint\")\n    rows = cursor.fetchall()\n    conn.close()\n    \n    return [row[0] for row in rows if row[0]]\n\n","path":null,"size_bytes":13356,"size_tokens":null},"group4/REFER/agent/agent.py":{"content":"","path":null,"size_bytes":0,"size_tokens":null},"group4/fim1/fim/alerts.py":{"content":"\"\"\"Console alert functions\"\"\"\r\nfrom datetime import datetime\r\n\r\n\r\ndef print_alert(event_type: str, file_path: str, endpoint: str, \r\n                hostname: str, username: str, timestamp: str = None) -> None:\r\n    \"\"\"Print a console alert for a file system event\r\n    \r\n    Args:\r\n        event_type: Type of event (created/modified/deleted)\r\n        file_path: Path to the file\r\n        endpoint: Endpoint identifier\r\n        hostname: Hostname\r\n        username: Username\r\n        timestamp: Optional timestamp string (defaults to current time)\r\n    \"\"\"\r\n    if timestamp is None:\r\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\r\n    \r\n    alert_type = event_type.upper()\r\n    \r\n    print(f\"[ALERT] {timestamp} {alert_type} {file_path} \"\r\n          f\"endpoint={endpoint} hostname={hostname} user={username}\")\r\n\r\n\r\n\r\n\r\n\r\n","path":null,"size_bytes":841,"size_tokens":null},"group4/fim1/fim/main.py":{"content":"\"\"\"Main entry point - runs watcher and Flask dashboard\"\"\"\r\nimport threading\r\nimport time\r\nfrom .models import init_db\r\nfrom .watcher import DirectoryWatcher\r\nfrom .app import run_app\r\nfrom .config import FLASK_HOST, FLASK_PORT\r\n\r\n\r\ndef main():\r\n    \"\"\"Initialize and run both the watcher and Flask app\"\"\"\r\n    # Initialize database\r\n    print(\"[INIT] Initializing database...\")\r\n    init_db()\r\n    print(\"[INIT] Database ready\")\r\n    \r\n    # Start directory watcher in a background thread\r\n    print(\"[INIT] Starting directory watcher...\")\r\n    watcher = DirectoryWatcher()\r\n    watcher_thread = threading.Thread(target=watcher.start, daemon=True)\r\n    watcher_thread.start()\r\n    \r\n    # Give watcher a moment to start\r\n    time.sleep(0.5)\r\n    \r\n    # Start Flask app (blocking)\r\n    try:\r\n        run_app(host=FLASK_HOST, port=FLASK_PORT, debug=False)\r\n    except KeyboardInterrupt:\r\n        print(\"\\n[SHUTDOWN] Shutting down...\")\r\n        watcher.stop()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n\r\n\r\n\r\n\r\n","path":null,"size_bytes":1013,"size_tokens":null},"group4/fim1/fim/watcher.py":{"content":"\"\"\"Directory watcher agent using watchdog\"\"\"\nimport os\nimport time\nfrom datetime import datetime\nimport socket\nimport getpass\nfrom pathlib import Path\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\n\nfrom .config import WATCH_DIRECTORY, ENDPOINT_NAME\nfrom .hashing import compute_hash\nfrom .models import insert_event, get_latest_hash\nfrom .alerts import print_alert\n\n\nclass FIMEventHandler(FileSystemEventHandler):\n    \"\"\"Handler for file system events\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        self.hostname = socket.gethostname()\n        self.username = getpass.getuser()\n        self.endpoint = ENDPOINT_NAME\n    \n    def _process_event(self, event_type: str, src_path: str, is_directory: bool = False) -> None:\n        \"\"\"Process a file system event\n        \n        Args:\n            event_type: Type of event (created/modified/deleted)\n            src_path: Path to the file or directory\n            is_directory: Whether the path is a directory\n        \"\"\"\n        # Skip directory events (only monitor files)\n        if is_directory:\n            return\n        \n        file_path = os.path.abspath(src_path)\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        \n        # Determine hash values based on event type\n        hash_before = None\n        hash_after = None\n        \n        if event_type == \"created\":\n            # Skip if not a file (might be a directory)\n            if not os.path.isfile(file_path):\n                return\n            hash_after = compute_hash(file_path)\n        \n        elif event_type == \"modified\":\n            # Skip if not a file\n            if not os.path.isfile(file_path):\n                return\n            hash_before = get_latest_hash(file_path)\n            hash_after = compute_hash(file_path)\n        \n        elif event_type == \"deleted\":\n            # For deleted files, we can't check if it's a file anymore\n            # Get hash_before from database (file is already deleted)\n            hash_before = get_latest_hash(file_path)\n            hash_after = None\n        \n        # Prepare event data\n        event_data = {\n            \"event_type\": event_type,\n            \"file_path\": file_path,\n            \"timestamp\": timestamp,\n            \"endpoint\": self.endpoint,\n            \"hostname\": self.hostname,\n            \"username\": self.username,\n            \"hash_before\": hash_before,\n            \"hash_after\": hash_after,\n        }\n        \n        # Store event in database\n        insert_event(event_data)\n        \n        # Print console alert\n        print_alert(\n            event_type=event_type,\n            file_path=file_path,\n            endpoint=self.endpoint,\n            hostname=self.hostname,\n            username=self.username,\n            timestamp=timestamp\n        )\n    \n    def on_created(self, event):\n        \"\"\"Handle file creation events\"\"\"\n        self._process_event(\"created\", event.src_path, event.is_directory)\n    \n    def on_modified(self, event):\n        \"\"\"Handle file modification events\"\"\"\n        self._process_event(\"modified\", event.src_path, event.is_directory)\n    \n    def on_deleted(self, event):\n        \"\"\"Handle file deletion events\"\"\"\n        self._process_event(\"deleted\", event.src_path, event.is_directory)\n\n\nclass DirectoryWatcher:\n    \"\"\"Directory watcher agent\"\"\"\n    \n    def __init__(self, watch_directory: str = None):\n        self.watch_directory = watch_directory or WATCH_DIRECTORY\n        self.observer = None\n        self.running = False\n    \n    def start(self) -> None:\n        \"\"\"Start watching the directory\"\"\"\n        # Ensure watched directory exists\n        os.makedirs(self.watch_directory, exist_ok=True)\n        \n        if self.running:\n            return\n        \n        event_handler = FIMEventHandler()\n        self.observer = Observer()\n        self.observer.schedule(event_handler, self.watch_directory, recursive=True)\n        self.observer.start()\n        self.running = True\n        \n        print(f\"[WATCHER] Started monitoring: {os.path.abspath(self.watch_directory)}\")\n    \n    def stop(self) -> None:\n        \"\"\"Stop watching the directory\"\"\"\n        if self.observer and self.running:\n            self.observer.stop()\n            self.observer.join()\n            self.running = False\n            print(\"[WATCHER] Stopped monitoring\")\n\n\ndef run_watcher(watch_directory: str = None) -> DirectoryWatcher:\n    \"\"\"Run the directory watcher (blocking)\n    \n    Args:\n        watch_directory: Optional directory to watch\n    \n    Returns:\n        DirectoryWatcher instance\n    \"\"\"\n    watcher = DirectoryWatcher(watch_directory)\n    watcher.start()\n    \n    try:\n        while True:\n            time.sleep(1)\n    except KeyboardInterrupt:\n        watcher.stop()\n    \n    return watcher\n\n","path":null,"size_bytes":4816,"size_tokens":null},"group4/fim1/README.md":{"content":"# File Integrity Monitoring (FIM) System\n\nA minimal viable product (MVP) for real-time file integrity monitoring in Python. This system monitors a directory for file changes (created, modified, deleted), computes SHA256 hashes, stores events in SQLite, and provides a Flask web dashboard.\n\n## Features\n\n- Real-time directory monitoring using watchdog\n- SHA256 hash computation for file integrity\n- SQLite database for event storage\n- Console alerts for file system events\n- Flask web dashboard with filtering and hash inspection\n\n## Project Structure\n\n```\n.\n├── README.md\n├── requirements.txt\n├── fim/\n│   ├── __init__.py\n│   ├── config.py          # Configuration settings\n│   ├── watcher.py         # Directory watcher agent\n│   ├── hashing.py         # SHA256 utilities\n│   ├── models.py          # SQLite database layer\n│   ├── alerts.py          # Console alert logic\n│   ├── app.py             # Flask application\n│   └── main.py            # Entry point\n├── templates/\n│   └── index.html         # Dashboard template\n└── data/\n    └── fim_events.db      # SQLite database (created on first run)\n```\n\n## Installation\n\n### 1. Create a virtual environment\n\n```bash\npython -m venv venv\n```\n\n### 2. Activate the virtual environment\n\nOn Windows:\n```bash\nvenv\\Scripts\\activate\n```\n\nOn Linux/Mac:\n```bash\nsource venv/bin/activate\n```\n\n### 3. Install dependencies\n\n```bash\npip install -r requirements.txt\n```\n\n## Usage\n\n### Running the Application\n\nStart the FIM system:\n\n```bash\npython main.py\n```\n\nOr:\n\n```bash\npython -m fim.main\n```\n\nThis will:\n- Initialize the SQLite database in `data/fim_events.db`\n- Start monitoring the directory specified in `fim/config.py`\n- Launch the Flask dashboard at `http://localhost:5000`\n\n### Configuration\n\nEdit `fim/config.py` to customize:\n\n- `WATCH_DIRECTORY`: Directory to monitor (default: `./watched`)\n- `DB_PATH`: SQLite database path (default: `./data/fim_events.db`)\n- `ENDPOINT_NAME`: Endpoint identifier for this agent (default: `local_agent`)\n- `FLASK_HOST`: Flask host (default: `0.0.0.0`)\n- `FLASK_PORT`: Flask port (default: `5000`)\n\n### Dashboard Features\n\n- **Filter by event type**: Use the dropdown to filter events (All, Created, Modified, Deleted)\n- **View event details**: See timestamp, event type, file path, endpoint, hostname, and username\n- **Inspect hashes**: Click the \"Info\" button on any row to view hash_before and hash_after values in a modal\n\n### Console Alerts\n\nThe system prints alerts to the console when events are detected:\n\n```\n[ALERT] 2025-11-28 16:33:22 MODIFIED /home/project/test.txt endpoint=local_agent hostname=myhost user=alice\n```\n\n## Database Schema\n\nThe `events` table contains:\n\n- `id`: Auto-incrementing primary key\n- `event_type`: created / modified / deleted\n- `file_path`: Absolute path to the file\n- `timestamp`: Local time string\n- `endpoint`: Endpoint identifier\n- `hostname`: System hostname\n- `username`: Current user\n- `hash_before`: Previous hash (for modified/deleted events)\n- `hash_after`: New hash (for created/modified events)\n\n## Notes\n\n- The watched directory will be created automatically if it doesn't exist\n- Only files are monitored (directories are ignored)\n- Large files are hashed in chunks for efficiency\n- The dashboard shows the latest 100 events by default\n\n","path":null,"size_bytes":3381,"size_tokens":null},"group4/fim1/fim/config.py":{"content":"\"\"\"Configuration settings for FIM system\"\"\"\r\nimport os\r\n\r\n# Directory to monitor\r\nWATCH_DIRECTORY = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"watched\")\r\n\r\n# Database path\r\nDATA_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"data\")\r\nDB_PATH = os.path.join(DATA_DIR, \"fim_events.db\")\r\n\r\n# Endpoint identifier for this agent\r\nENDPOINT_NAME = \"local_agent\"\r\n\r\n# Flask settings\r\nFLASK_HOST = \"0.0.0.0\"\r\nFLASK_PORT = 5000\r\n\r\n\r\n\r\n\r\n\r\n","path":null,"size_bytes":460,"size_tokens":null},"group4/fim1/main.py":{"content":"\"\"\"Main entry point - run with: python main.py\"\"\"\r\nfrom fim.main import main\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n\r\n\r\n\r\n\r\n","path":null,"size_bytes":130,"size_tokens":null},"group4/fim1/fim/__init__.py":{"content":"\"\"\"File Integrity Monitoring System\"\"\"\r\n\r\n__version__ = \"1.0.0\"\r\n\r\n\r\n\r\n\r\n\r\n","path":null,"size_bytes":75,"size_tokens":null},"group4/fim1/fim/hashing.py":{"content":"\"\"\"SHA256 hashing utilities\"\"\"\r\nimport hashlib\r\nimport os\r\nfrom typing import Optional\r\n\r\n\r\ndef compute_hash(file_path: str) -> Optional[str]:\r\n    \"\"\"Compute SHA256 hash of a file\r\n    \r\n    Args:\r\n        file_path: Path to the file\r\n    \r\n    Returns:\r\n        Hex digest of SHA256 hash, or None if file cannot be read\r\n    \"\"\"\r\n    if not os.path.exists(file_path) or not os.path.isfile(file_path):\r\n        return None\r\n    \r\n    try:\r\n        sha256_hash = hashlib.sha256()\r\n        with open(file_path, \"rb\") as f:\r\n            # Read file in chunks to handle large files\r\n            for byte_block in iter(lambda: f.read(4096), b\"\"):\r\n                sha256_hash.update(byte_block)\r\n        return sha256_hash.hexdigest()\r\n    except (IOError, OSError, PermissionError):\r\n        return None\r\n\r\n\r\n\r\n\r\n\r\n","path":null,"size_bytes":812,"size_tokens":null}},"version":2}